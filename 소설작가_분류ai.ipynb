{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "소설작가 분류ai",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12nSCCYI33CXy4OtqfRYCkujR-mEuMXl5",
      "authorship_tag": "ABX9TyMQ/htQ+re6RZlp508RRJQL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gihoonpark/Novelist_classification-NLP/blob/main/%EC%86%8C%EC%84%A4%EC%9E%91%EA%B0%80_%EB%B6%84%EB%A5%98ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kVl0Ge3fix5"
      },
      "source": [
        "!pip install tensorflow==1.14.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8v0zZ2I35ip"
      },
      "source": [
        "!pip install tensorflow-gpu==1.14  # GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmu_ldEHHi4I"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5qU_-j3l5BT"
      },
      "source": [
        "Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoIBQG9vwKaE"
      },
      "source": [
        "from matplotlib import rcParams, pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional, Input, Concatenate\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "import warnings \n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p1Nstk59wOJ"
      },
      "source": [
        "# 코사인 유사도, 로지스틱\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gMJcGcPmLUs"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-TsvEM8wYJb"
      },
      "source": [
        "pd.set_option('max_columns', 100)\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twK1rOVkl7yQ"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HnVyeTMwYL6"
      },
      "source": [
        "trn_file = '/content/drive/MyDrive/dataset/소설작가 ai/train.csv'\n",
        "tst_file = '/content/drive/MyDrive/dataset/소설작가 ai/test_x.csv'\n",
        "sample_file = '/content/drive/MyDrive/dataset/소설작가 ai/sample_submission.csv'\n",
        "\n",
        "target_col = 'author'\n",
        "n_fold = 5\n",
        "n_class = 5\n",
        "seed = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3iyTeUrwYQ8"
      },
      "source": [
        "train = pd.read_csv(trn_file, index_col=0)\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqyLl2t0wYTa"
      },
      "source": [
        "test = pd.read_csv(tst_file, index_col=0)\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZsbppTm_qPU"
      },
      "source": [
        "# 코사인 유사도, 로지스틱\n",
        "trn = train\n",
        "tst = test\n",
        "\n",
        "trn['text'] = trn['text'].str.lower().apply(alpha_num)\n",
        "tst['text'] = tst['text'].str.lower().apply(alpha_num)\n",
        "\n",
        "s = str(trn.text[:])\n",
        "tokens = word_tokenize(s)\n",
        "print(tokens)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
        "X = vec.fit_transform(trn['text'])\n",
        "X_tst = vec.transform(tst['text'])\n",
        "print(X.shape, X_tst.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgmGFcDZ_TTE"
      },
      "source": [
        "# 코사인 유사도, 로지스틱\n",
        "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
        "y = trn.author.values\n",
        "p = np.zeros((X.shape[0], n_class))\n",
        "p_tst = np.zeros((X_tst.shape[0], n_class))\n",
        "for i_cv, (i_trn, i_val) in enumerate(cv.split(X, y), 1):\n",
        "    print(f'training model for CV #{i_cv}')\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(X[i_trn], y[i_trn])\n",
        "    p[i_val, :] = clf.predict_proba(X[i_val])\n",
        "    p_tst += clf.predict_proba(X_tst) / n_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y98jtA3N_bNg"
      },
      "source": [
        "# 코사인 유사도, 로지스틱\n",
        "print(f'Accuracy (CV): {accuracy_score(y, np.argmax(p, axis=1)) * 100:8.4f}%')\n",
        "print(f'Log Loss (CV): {log_loss(pd.get_dummies(y), p):8.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7_1vD3-ISKB"
      },
      "source": [
        "def remove_stopwords_nltk(text): # 2.불용어 제거\n",
        "    final_text = []\n",
        "    for i in text.split(): # text를 띄어쓰기 기준으로 원소로 나누고, 하나 씩 불용어와 비교, 불용어가 아니면 final_text에 하나씩 추가.\n",
        "        if i.strip().lower() not in stop_words: # 원소를 소문자로 바꾼 후, 불용어와 비교.\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text) # 띄어쓰기 기준으로 나눈 것을 다시 문장으로 연결하여 출력."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agSJ3IyWw9wY"
      },
      "source": [
        "preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djIcKHNDwimg"
      },
      "source": [
        "def alpha_num(text):\n",
        "    return re.sub(r'[^A-Za-z0-9 ]', '', text) # 1.정제 : [^A-Za-z0-9 ]빼고 전부 없애기. -> r'[^A-Za-z0-9 ]'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W61ugcjpwipS"
      },
      "source": [
        "train['text'] = train['text'].str.lower().apply(alpha_num).apply(remove_stopwords_nltk)\n",
        "test['text'] = test['text'].str.lower().apply(alpha_num).apply(remove_stopwords_nltk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipvawWSJwirj"
      },
      "source": [
        "X_train = train['text'].values\n",
        "X_test = test['text'].values\n",
        "y = train['author'].values\n",
        "print(X_train.shape, X_test.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMd6gE5Awit0"
      },
      "source": [
        "X_train[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5c3UI66wYV6"
      },
      "source": [
        "vocab_size = 20000\n",
        "embedding_dim = 64\n",
        "max_length = 505\n",
        "padding_type='post'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzcNS8uswYYk"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)# 토큰화 후\n",
        "word_index = tokenizer.word_index # 정수 인코딩"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSimqL6CwYbO"
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ngMHJdwtmF"
      },
      "source": [
        "trn = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length) # 모든 입력 데이터 크기 통일시키기(padding 방법, max 길이 설정)\n",
        "tst = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length) \n",
        "print(trn.shape, tst.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHLQw14YnCoc"
      },
      "source": [
        "Model & Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyQ2PCIJwto4"
      },
      "source": [
        "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TReFuW2heywc"
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = Dense(units)\n",
        "    self.W2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "\n",
        "  def call(self, values, query):\n",
        "    # query shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY7NAgD_wtrP"
      },
      "source": [
        "def get_model():\n",
        "    sequence_input = Input(shape=(max_length,), dtype='int32')\n",
        "    embedded_sequences = Embedding(vocab_size, embedding_dim, input_length=max_length, mask_zero = True)(sequence_input)\n",
        "    lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(embedded_sequences)\n",
        "    \n",
        "    state_h = Concatenate()([forward_h, backward_h]) # 은닉 상태\n",
        "    state_c = Concatenate()([forward_c, backward_c]) # 셀 상태\n",
        "    attention = BahdanauAttention(64) # 가중치 크기 정의\n",
        "    context_vector, attention_weights = attention(lstm, state_h)\n",
        "    \n",
        "    output = Dense(n_class, activation=\"softmax\")(context_vector)\n",
        "    model = Model(inputs=sequence_input, outputs=output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=.01))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd4as19lXbNB"
      },
      "source": [
        "p_val = np.zeros((trn.shape[0], n_class))\n",
        "p_tst = np.zeros((tst.shape[0], n_class))\n",
        "for i, (i_trn, i_val) in enumerate(cv.split(trn, y), 1): # cv\n",
        "\n",
        "    print(f'training model for CV #{i}')\n",
        "    #clf = get_model()\n",
        "    es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
        "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "    sequence_input = Input(shape=(max_length,), dtype='int32')\n",
        "    embedded_sequences = Embedding(vocab_size, embedding_dim, input_length=max_length, mask_zero = True)(sequence_input)\n",
        "    lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(embedded_sequences)\n",
        "    \n",
        "    state_h = Concatenate()([forward_h, backward_h]) # 은닉 상태\n",
        "    state_c = Concatenate()([forward_c, backward_c]) # 셀 상태\n",
        "    attention = BahdanauAttention(64) # 가중치 크기 정의\n",
        "    context_vector, attention_weights = attention(lstm, state_h)\n",
        "\n",
        "    dense = Dense(20, activation=\"relu\")(context_vector)\n",
        "    output = Dense(n_class, activation=\"softmax\")(dense)\n",
        "    model = Model(inputs=sequence_input, outputs=output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=.01))\n",
        "    model.fit(trn[i_trn], \n",
        "            to_categorical(y[i_trn]),\n",
        "            validation_data=(trn[i_val], to_categorical(y[i_val])),\n",
        "            epochs=5,\n",
        "            batch_size=512,\n",
        "            callbacks=[es])\n",
        "\n",
        "    p_val[i_val, :] = model.predict(trn[i_val])\n",
        "    p_tst += model.predict(tst) / n_fold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIP_0WN_wYdl"
      },
      "source": [
        "print(f'Accuracy (CV): {accuracy_score(y, np.argmax(p_val, axis=1)) * 100 :8.4f}%')\n",
        "print(f'Log Loss (CV): {log_loss(pd.get_dummies(y), p_val):8.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBd-FHy4nIQx"
      },
      "source": [
        "Submit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYSij8ETEqKU"
      },
      "source": [
        "df1 = pd.read_csv('/content/drive/MyDrive/dataset/소설작가 ai/submission_소설작가1.csv', index_col=0)\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/dataset/소설작가 ai/submission_소설작가2.csv', index_col=0)\n",
        "df3 = pd.read_csv('/content/drive/MyDrive/dataset/소설작가 ai/submission_소설작가3.csv', index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T9McuVpE6Hl"
      },
      "source": [
        "df4 = (df1 + df2 + df3) / 3\n",
        "df4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDBJ2VeTFagM"
      },
      "source": [
        "df4.to_csv(sample_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mKy2i3Dw1D5"
      },
      "source": [
        "sub = pd.read_csv(sample_file, index_col=0)\n",
        "print(sub.shape)\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abrCkr5Uw1Gv"
      },
      "source": [
        "sub[sub.columns] = p_tst\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmdNarqvw1JI"
      },
      "source": [
        "sub.to_csv(sample_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}